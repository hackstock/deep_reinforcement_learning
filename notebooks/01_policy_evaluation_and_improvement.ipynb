{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDP for bandit_walk_five : \n",
    "# Deterministic environment (100% action success)\n",
    "# 3 non-terminal states, 2 terminal states\n",
    "# The only reward (+1) is at the right-most state\n",
    "# Episodic environment where agent terminates in the \n",
    "# left-most or right-most states T-1-2-3-T\n",
    "# actions left (0), right (1)\n",
    "bandit_walk = {\n",
    "    0: {\n",
    "        0: [(1.0,0,0.0,True)],\n",
    "        1: [(1.0,0,0.0,True)]\n",
    "    },\n",
    "    1: {\n",
    "        0: [(1.0,0,0.0,True)],\n",
    "        1: [(1.0,2,0.0,False)]\n",
    "    },\n",
    "    2: {\n",
    "        0: [(1.0,1,0.0,False)],\n",
    "        1: [(1.0,3,0.0,False)]\n",
    "    },\n",
    "    3: {\n",
    "        0: [(1.0,2,0.0,False)],\n",
    "        1: [(1.0,4,1.0,True)]\n",
    "    },\n",
    "    4: {\n",
    "        0: [(1.0,4,0.0,True)],\n",
    "        1: [(1.0,4,0.0,True)]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDP for slippery_bandit_walk_five : \n",
    "# Stochastic environment (80% action success, 20% backwards)\n",
    "# 3 non-terminal states, 2 terminal states\n",
    "# The only reward (+1) is at the right-most state\n",
    "# Episodic environment where agent terminates in the \n",
    "# left-most or right-most states T-1-2-3-T\n",
    "# actions left (0), right (1)\n",
    "slippery_bandit_walk = {\n",
    "    0: {\n",
    "        0: [(1.0,0,0.0,True)],\n",
    "        1: [(1.0,0,0.0,True)]\n",
    "    },\n",
    "    1: {\n",
    "        0: [(0.8,0,0.0,True),(0.2,2,0.0,False)],\n",
    "        1: [(0.8,2,0.0,False),(0.2,0,0.0,True)]\n",
    "    },\n",
    "    2: {\n",
    "        0: [(0.8,1,0.0,False),(0.2,3,0.0,False)],\n",
    "        1: [(0.8,3,0.0,False),(0.2,1,0.0,False)]\n",
    "    },\n",
    "    3: {\n",
    "        0: [(0.8,2,0.0,False),(0.2,4,1.0,True)],\n",
    "        1: [(0.8,4,1.0,True),(0.2,2,0.0,False)]\n",
    "    },\n",
    "    4: {\n",
    "        0: [(1.0,4,0.0,True)],\n",
    "        1: [(1.0,4,0.0,True)]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Walk: \n",
    "# highly stochastic environment (50% action success, 50% backwards)\n",
    "# 5 non-terminal states, 2 terminal states\n",
    "# only reward is still at the right-most cell in the \"walk\"\n",
    "# episodic environment, the agent terminates at the left- or right-most cell\n",
    "# agent starts in state 3 (middle of the walk) T-1-2-3-4-5-T\n",
    "# actions left (0) or right (1), which don't really make a difference since walk is random\n",
    "random_walk = {\n",
    "    0: {\n",
    "        0: [(1.0, 0, 0.0, True)],\n",
    "        1: [(1.0, 0, 0.0, True)]\n",
    "    },\n",
    "    1: {\n",
    "        0: [(0.5, 0, 0.0, True), (0.5, 2, 0.0, False)],\n",
    "        1: [(0.5, 2, 0.0, False), (0.5, 0, 0.0, True)]\n",
    "    },\n",
    "    2: {\n",
    "        0: [(0.5, 1, 0.0, False), (0.5, 3, 0.0, False)],\n",
    "        1: [(0.5, 3, 0.0, False), (0.5, 1, 0.0, False)]\n",
    "    },\n",
    "    3: {\n",
    "        0: [(0.5, 2, 0.0, False), (0.5, 4, 0.0, False)],\n",
    "        1: [(0.5, 4, 0.0, False), (0.5, 2, 0.0, False)]\n",
    "    },\n",
    "    4: {\n",
    "        0: [(0.5, 3, 0.0, False), (0.5, 5, 0.0, False)],\n",
    "        1: [(0.5, 5, 0.0, False), (0.5, 3, 0.0, False)]\n",
    "    },\n",
    "    5: {\n",
    "        0: [(0.5, 4, 0.0, False), (0.5, 6, 1.0, True)],\n",
    "        1: [(0.5, 6, 1.0, True), (0.5, 4, 0.0, False)]\n",
    "    },\n",
    "    6: {\n",
    "        0: [(1.0, 6, 0.0, True)],\n",
    "        1: [(1.0, 6, 0.0, True)]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {0: [(1.0, 0, 0.0, True)], 1: [(1.0, 0, 0.0, True)]}, 1: {0: [(1.0, 0, 0.0, True)], 1: [(1.0, 2, 0.0, False)]}, 2: {0: [(1.0, 1, 0.0, False)], 1: [(1.0, 3, 0.0, False)]}, 3: {0: [(1.0, 2, 0.0, False)], 1: [(1.0, 4, 1.0, True)]}, 4: {0: [(1.0, 4, 0.0, True)], 1: [(1.0, 4, 0.0, True)]}}\n"
     ]
    }
   ],
   "source": [
    "print(bandit_walk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {0: [(1.0, 0, 0.0, True)], 1: [(1.0, 0, 0.0, True)]}, 1: {0: [(0.8, 0, 0.0, True), (0.2, 2, 0.0, False)], 1: [(0.8, 2, 0.0, False), (0.2, 0, 0.0, True)]}, 2: {0: [(0.8, 1, 0.0, False), (0.2, 3, 0.0, False)], 1: [(0.8, 3, 0.0, False), (0.2, 1, 0.0, False)]}, 3: {0: [(0.8, 2, 0.0, False), (0.2, 4, 1.0, True)], 1: [(0.8, 4, 1.0, True), (0.2, 2, 0.0, False)]}, 4: {0: [(1.0, 4, 0.0, True)], 1: [(1.0, 4, 0.0, True)]}}\n"
     ]
    }
   ],
   "source": [
    "print(slippery_bandit_walk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {0: [(1.0, 0, 0.0, True)], 1: [(1.0, 0, 0.0, True)]}, 1: {0: [(0.5, 0, 0.0, True), (0.5, 2, 0.0, False)], 1: [(0.5, 2, 0.0, False), (0.5, 0, 0.0, True)]}, 2: {0: [(0.5, 1, 0.0, False), (0.5, 3, 0.0, False)], 1: [(0.5, 3, 0.0, False), (0.5, 1, 0.0, False)]}, 3: {0: [(0.5, 2, 0.0, False), (0.5, 4, 0.0, False)], 1: [(0.5, 4, 0.0, False), (0.5, 2, 0.0, False)]}, 4: {0: [(0.5, 3, 0.0, False), (0.5, 5, 0.0, False)], 1: [(0.5, 5, 0.0, False), (0.5, 3, 0.0, False)]}, 5: {0: [(0.5, 4, 0.0, False), (0.5, 6, 1.0, True)], 1: [(0.5, 6, 1.0, True), (0.5, 4, 0.0, False)]}, 6: {0: [(1.0, 6, 0.0, True)], 1: [(1.0, 6, 0.0, True)]}}\n"
     ]
    }
   ],
   "source": [
    "print(random_walk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(pi, P, gamma=1.0, theta=1e-10):\n",
    "    state_space = len(P)\n",
    "    prev_state_values = np.zeros(state_space)\n",
    "    \n",
    "    while True:\n",
    "        current_state_values = np.zeros(state_space)\n",
    "        for state in range(state_space):\n",
    "            for prob, next_state, reward, done in P[state][pi[state]]:\n",
    "                current_state_values[state] += prob * (reward + gamma * prev_state_values[next_state] * (not done))\n",
    "        if np.max(np.abs(prev_state_values - current_state_values)) < theta:\n",
    "            break\n",
    "        prev_state_values = current_state_values.copy()\n",
    "    return current_state_values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_always_right = {0:1,1:1,2:1,3:1,4:1}\n",
    "policy_always_left = {0:0,1:0,2:0,3:0,4:0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_values_always_right = policy_evaluation(policy_always_right, bandit_walk, gamma=0.99)\n",
    "state_values_always_left = policy_evaluation(policy_always_left, bandit_walk, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALWAYS RIGHT : [0.     0.9801 0.99   1.     0.    ]\n",
      "ALWAYS LEFT : [0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(f\"ALWAYS RIGHT : {state_values_always_right}\")\n",
    "print(f\"ALWAYS LEFT : {state_values_always_left}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_values_always_right = policy_evaluation(policy_always_right, slippery_bandit_walk, gamma=0.99)\n",
    "state_values_always_left = policy_evaluation(policy_always_left, slippery_bandit_walk, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALWAYS RIGHT : [0.         0.73111101 0.92311996 0.98277775 0.        ]\n",
      "ALWAYS LEFT : [0.         0.01142361 0.057695   0.24569444 0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(f\"ALWAYS RIGHT : {state_values_always_right}\")\n",
    "print(f\"ALWAYS LEFT : {state_values_always_left}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(V,P, gamma=1.0):\n",
    "    state_space = len(P)\n",
    "    action_space = len(P[0])\n",
    "    \n",
    "    Q = np.zeros((state_space, action_space))\n",
    "    \n",
    "    for state in range(state_space):\n",
    "        for action in range(action_space):\n",
    "            for prob, next_state, reward, done in P[state][action]:\n",
    "                Q[state][action] = prob * (reward + gamma * V[next_state] * (not done))\n",
    "    \n",
    "    new_pi = {s:a for s,a in enumerate(np.argmax(Q, axis=1))}\n",
    "    return new_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_policy_always_right = policy_improvement(state_values_always_left,slippery_bandit_walk, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 0, 2: 0, 3: 0, 4: 0}\n"
     ]
    }
   ],
   "source": [
    "print(improved_policy_always_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(P, gamma=1.0, theta=1e-10):\n",
    "    state_space = len(P)\n",
    "    action_space = len(P[0])\n",
    "    \n",
    "    random_actions = [np.random.choice(action_space) for _ in range(state_space)]\n",
    "    pi = {s:a for s,a in enumerate(random_actions)}\n",
    "    print(f\"RANDOM POLICY : {pi}\")\n",
    "    \n",
    "    while True:\n",
    "        old_pi = pi.copy()\n",
    "        V = policy_evaluation(pi, P, gamma, theta)\n",
    "        pi = policy_improvement(V,P,gamma)\n",
    "        if old_pi == pi:\n",
    "            break\n",
    "            \n",
    "    return V, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM POLICY : {0: 0, 1: 0, 2: 1, 3: 1, 4: 0}\n"
     ]
    }
   ],
   "source": [
    "state_values, policy = policy_iteration(bandit_walk, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATE VALUES : [0.     0.9801 0.99   1.     0.    ], POLICY : {0: 0, 1: 1, 2: 1, 3: 1, 4: 0}\n"
     ]
    }
   ],
   "source": [
    "print(f\"STATE VALUES : {state_values}, POLICY : {policy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(P, gamma=1.0, theta=1e-10):\n",
    "    state_space = len(P)\n",
    "    action_space = len(P[0])\n",
    "    \n",
    "    V = np.zeros(state_space, dtype=np.float64)\n",
    "    while True:\n",
    "        Q = np.zeros((state_space, action_space), dtype=np.float64)\n",
    "        for state in range(state_space):\n",
    "            for action in range(action_space):\n",
    "                for prob, next_state, reward, done in P[state][action]:\n",
    "                    Q[state][action] += prob * (reward + gamma * V[next_state] * (not done))\n",
    "        if np.max(np.abs(V - np.max(Q, axis=1))) < theta:\n",
    "            break\n",
    "            \n",
    "        V = np.max(Q, axis=1)\n",
    "        pi = {s:a for s,a in enumerate(np.argmax(Q, axis=1))}\n",
    "        \n",
    "        return V,pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATE VALUES : [0.     0.9801 0.99   1.     0.    ], POLICY : {0: 0, 1: 0, 2: 0, 3: 1, 4: 0}\n"
     ]
    }
   ],
   "source": [
    "state_value, policy = value_iteration(bandit_walk, gamma=0.99)\n",
    "print(f\"STATE VALUES : {state_values}, POLICY : {policy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
